load("E:/Users/LENOVO/Documents/KMMI_R/Pertemuan11/.RData")
library(readxl)
#Import data
data <- read.csv("Kel8.csv")
library(tidytext)
library(dplyr)
library(stringr)
library(tm)
library(stopwords)
library(katadasaR)
library(stringi)
library(qdapRegex)
library(igraph)
library(networkD3)
library(readxl)
# kata normalisasi
old <- c(" ni "," tu "," org "," yg "," ga ","ambik"," klo "," ngga ","tdk", " nggak ", " ya ","p")
new <- c(" ini "," itu "," orang "," yang "," tidak ","ambil"," kalau "," tidak "," tidak ", " tidak "," iya ","pak")
#butuh dihapus
hapus <- c("pak","erick","najwa")
fungsi_proses <- function(a){
a %>%
casefold() %>%
rm_url(pattern = pastex("@rm_twitter_url","@rm_url")) %>%
gsub('#\\S+','',.) %>%
gsub('@\\S+','',.) %>%
gsub('[^[:alpha:] ]','',.) %>%
stri_trans_general("latin-ascii") %>%
stri_replace_all_fixed(old,new,vectorize_all = FALSE) %>%
removeWords(stopwords(language = "id", source = "nltk")) %>%
removeWords(hapus)
}
datapro <- mutate(data,textDisplay=sapply(text,fungsi_proses))
datanew <- mutate(data,komentar_bersih=sapply(textDisplay,fungsi_proses))
View(datanew)
kata normalisasi
fungsi_proses <- function(a){
a %>%
casefold() %>%
rm_url(pattern = pastex("@rm_twitter_url","@rm_url")) %>%
gsub('#\\S+','',.) %>%
gsub('@\\S+','',.) %>%
gsub('[^[:alpha:] ]','',.) %>%
stri_trans_general("latin-ascii") %>%
stri_replace_all_fixed(old,new,vectorize_all = FALSE) %>%
removeWords(stopwords(language = "id", source = "nltk")) %>%
removeWords(hapus)
}
datanew <- mutate(data,komentar_bersih=sapply(textDisplay,fungsi_proses))
View(datanew)
View(datanew)
View(datanew)
library(dplyr)
library(stringr)
library(tm)
library(stopwords)
library(katadasaR)
library(stringi)
library(qdapRegex)
library(igraph)
library(networkD3)
library(readxl)
# kata normalisasi
old <- c(" ni "," tu "," org "," yg "," ga ","ambik"," klo "," ngga ","tdk", " nggak ", " ya ")
new <- c(" ini "," itu "," orang "," yang "," tidak ","ambil"," kalau "," tidak "," tidak ", " tidak "," iya ")
fungsi_proses <- function(a){
a %>%
casefold() %>%
gsub('#\\S+','',.) %>%
gsub('@\\S+','',.) %>%
gsub('[^[:alpha:] ]','',.) %>%
stri_trans_general("latin-ascii") %>%
stri_replace_all_fixed(old,new,vectorize_all = FALSE) %>%
removeWords(stopwords(language = "id", source = "nltk"))
}
read.csv("Kel8.csv")
data <- read.csv("Kel8.csv")
View(data)
library(dplyr)
library(stringr)
library(tm)
library(stopwords)
library(katadasaR)
library(stringi)
library(qdapRegex)
library(igraph)
library(networkD3)
library(readxl)
# kata normalisasi
old <- c(" ni "," tu "," org "," yg "," ga ","ambik"," klo "," ngga ","tdk", " nggak ", " ya ")
new <- c(" ini "," itu "," orang "," yang "," tidak ","ambil"," kalau "," tidak "," tidak ", " tidak "," iya ")
fungsi_proses <- function(a){
a %>%
casefold() %>%
rm_url(pattern = pastex("@rm_twitter_url","@rm_url")) %>%
gsub('#\\S+','',.) %>%
gsub('@\\S+','',.) %>%
gsub('[^[:alpha:] ]','',.) %>%
stri_trans_general("latin-ascii") %>%
stri_replace_all_fixed(old,new,vectorize_all = FALSE) %>%
removeWords(stopwords(language = "id", source = "nltk"))
}
fungsi_proses <- function(a){
a %>%
casefold() %>%
gsub('#\\S+','',.) %>%
gsub('@\\S+','',.) %>%
gsub('[^[:alpha:] ]','',.) %>%
stri_trans_general("latin-ascii") %>%
stri_replace_all_fixed(old,new,vectorize_all = FALSE) %>%
removeWords(stopwords(language = "id", source = "nltk"))
}
View(fungsi_proses)
data <- read.csv("Kel8.csv")
dtnew <- mutate(data,komentar_bersih=sapply(textDisplay,fungsi_proses))
View(dtnew)
dttoken <- dtnew %>%
select(komentar_bersih) %>%
unnest_tokens(kata,komentar_bersih) %>%
count(kata) %>%
top_n(30)
library(tidytext)
dttoken <- dtnew %>%
select(komentar_bersih) %>%
unnest_tokens(kata,komentar_bersih) %>%
count(kata) %>%
top_n(30)
View(dttoken)
library(ggplot2)
library(ggplot2)
ggplot(dttoken,aes(x=reorder(kata,n),y=n)) + geom_col() + coord_flip()
install.packages("ggplot")
library(ggplot2)
gplot(dttoken,aes(x=reorder(kata,n),y=n)) + geom_col() + coord_flip()
ggplot(dttoken,aes(x=reorder(kata,n),y=n)) + geom_col() + coord_flip()
library(ggplot)
library(ggplot2)
library(wordcloud2)
dt1 <- dt %>% select(AuhorDisplayName,textDisplay)
library(dplyr)
library(stringr)
library(tm)
library(stopwords)
library(katadasaR)
library(stringi)
library(qdapRegex)
library(igraph)
library(networkD3)
library(readxl)
library(tidytext)
dt1 <- dt %>% select(AuhorDisplayName,textDisplay)
library(writexl)
dt1 <- dt %>% select(AuhorDisplayName,textDisplay)
library(tidyr)
dt1 <- dt %>% select(AuhorDisplayName,textDisplay)
library(dplyr)
library(stringr)
library(tm)
library(stopwords)
library(katadasaR)
library(stringi)
library(qdapRegex)
library(igraph)
library(networkD3
dt1 <- dt %>% select(AuhorDisplayName,textDisplay
dt1 <- dt %>% select(AuhorDisplayName,textDisplay)
fungsi_proses <- function(a){
a %>%
casefold() %>%
rm_url(pattern = pastex("@rm_youtube_url","@rm_url")) %>%
gsub('#\\S+','',.) %>%
gsub('@\\S+','',.) %>%
gsub('[^[:alpha:] ]','',.) %>%
stri_trans_general("latin-ascii") %>%
stri_replace_all_fixed(old,new,vectorize_all = FALSE) %>%
removeWords(stopwords(language = "id", source = "nltk"))
}
dt1 <- dt %>% select(AuhorDisplayName,textDisplay)
dt1 <- data %>% select(AuhorDisplayName,textDisplay)
dt1 <- data %>% select(authorDisplayName,textDisplay)
library(ggplot2)
ggplot(dttoken,aes(x=reorder(kata,n),y=n)) + geom_col() + coord_flip()
dttoken <- dtnew %>%
select(komentar_bersih) %>%
unnest_tokens(kata,komentar_bersih) %>%
count(kata) %>%
top_n(10)
View(dttoken)
dttoken <- dtnew %>%
select(komentar_bersih) %>%
unnest_tokens(kata,komentar_bersih) %>%
count(kata) %>%
top_n(12)
hapus <- c("aja","erick","erik", "gak","nya")
fungsi_proses <- function(a){
a %>%
casefold() %>%
rm_url(pattern = pastex("@rm_youtube_url","@rm_url")) %>%
gsub('#\\S+','',.) %>%
gsub('@\\S+','',.) %>%
gsub('[^[:alpha:] ]','',.) %>%
stri_trans_general("latin-ascii") %>%
stri_replace_all_fixed(old,new,vectorize_all = FALSE) %>%
removeWords(stopwords(language = "id", source = "nltk"))
removeWords(hapus)
}
dtnew <- mutate(data,komentar_bersih=sapply(textDisplay,fungsi_proses))
dtnew <- mutate(data,komentar_bersih=sapply(textDisplay,fungsi_proses))
fungsi_proses <- function(a){
a %>%
casefold() %>%
rm_url(pattern = pastex("@rm_youtube_url","@rm_url")) %>%
gsub('#\\S+','',.) %>%
gsub('@\\S+','',.) %>%
gsub('[^[:alpha:] ]','',.) %>%
stri_trans_general("latin-ascii") %>%
stri_replace_all_fixed(old,new,vectorize_all = FALSE) %>%
removeWords(stopwords(language = "id", source = "nltk")) %>%
removeWords(hapus)
}
dtnew <- mutate(data,komentar_bersih=sapply(textDisplay,fungsi_proses))
dttoken <- dtnew %>%
select(komentar_bersih) %>%
unnest_tokens(kata,komentar_bersih) %>%
count(kata) %>%
top_n(12)
dttoken <- dtnew %>%
select(komentar_bersih) %>%
unnest_tokens(kata,komentar_bersih) %>%
count(kata) %>%
top_n(12)
hapus <- c("aja","erick","erik", "gak","nya","iya")
fungsi_proses <- function(a){
a %>%
casefold() %>%
rm_url(pattern = pastex("@rm_youtube_url","@rm_url")) %>%
gsub('#\\S+','',.) %>%
gsub('@\\S+','',.) %>%
gsub('[^[:alpha:] ]','',.) %>%
stri_trans_general("latin-ascii") %>%
stri_replace_all_fixed(old,new,vectorize_all = FALSE) %>%
removeWords(stopwords(language = "id", source = "nltk")) %>%
removeWords(hapus)
}
dtnew <- mutate(data,komentar_bersih=sapply(textDisplay,fungsi_proses))
dttoken <- dtnew %>%
select(komentar_bersih) %>%
unnest_tokens(kata,komentar_bersih) %>%
count(kata) %>%
top_n(12)
dttoken <- dtnew %>%
select(komentar_bersih) %>%
unnest_tokens(kata,komentar_bersih) %>%
count(kata) %>%
top_n(15)
View(dttoken)
hapus <- c("aja","erick","erik", "gak","nya","iya","orang","semoga","tohir")
fungsi_proses <- function(a){
a %>%
casefold() %>%
rm_url(pattern = pastex("@rm_youtube_url","@rm_url")) %>%
gsub('#\\S+','',.) %>%
gsub('@\\S+','',.) %>%
gsub('[^[:alpha:] ]','',.) %>%
stri_trans_general("latin-ascii") %>%
stri_replace_all_fixed(old,new,vectorize_all = FALSE) %>%
removeWords(stopwords(language = "id", source = "nltk")) %>%
removeWords(hapus)
}
dtnew <- mutate(data,komentar_bersih=sapply(textDisplay,fungsi_proses))
dttoken <- dtnew %>%
select(komentar_bersih) %>%
unnest_tokens(kata,komentar_bersih) %>%
count(kata) %>%
top_n(10)
dttoken <- dtnew %>%
select(komentar_bersih) %>%
unnest_tokens(kata,komentar_bersih) %>%
count(kata) %>%
top_n(12)
dt1$authorDisplayName <- paste0('@',dt1$authorDisplayName)
View(dt1)
dt2 <- dt1 %>%
unite(teks,authorDisplayName,text,sep=" ")
dt2 <- dt1 %>%
unite(teks,authorDisplayName,textDisplay,sep=" ")
View(dt2)
dt3 <- str_extract_all(dt2$teks,"(@[[:alnum:]_]*)")
dt3 <- sapply(dt3,paste,collapse=" ")
dt3 <- data.frame(dt3)
View(dt3)
dt3$count <- str_count(dt3$dt3,"\\S+")
dt4 <- dt3 %>% filter(count > 1)
dt5 <- dt4 %>%
select(dt3) %>%
unnest_tokens(username,dt3,token = "ngrams",n=2)
dt6 <- dt5 %>%
separate(username,into=c("source","target"),sep=" ")
dt6$source <- paste0('@',dt6$source)
dt6$target <- paste0('@',dt6$target)
ig <- graph_from_data_frame(dt6,directed=FALSE)
plot(ig,layout=layout_with_kk,vertex.size=3,vertex.label=NA)
library(networkD3)
simpleNetwork(dt6)
View(dt6)
View(dt5)
write_graph(ig,"ajsk8.graphml",format="graphml")
View(data)
View(dt1)
View(dt6)
View(dt6)
View(data)
View(dt1)
View(dt3)
View(dt5)
View(dt6)
View(dttoken)
View(ig)
View(dtnew)
library(dplyr)
library(stringr)
library(tm)
library(stopwords)
library(katadasaR)
library(stringi)
library(qdapRegex)
library(igraph)
library(networkD3)
library(readxl)
library(tidytext)
library(writexl)
library(tidyr)
# kata normalisasi
old <- c(" ni "," tu "," org "," yg "," ga ","ambik"," klo "," ngga ","tdk", " nggak ", " ya ")
new <- c(" ini "," itu "," orang "," yang "," tidak ","ambil"," kalau "," tidak "," tidak ", " tidak "," iya ")
#butuh dihapus
hapus <- c("aja","erick","erik", "gak","nya","iya","orang","semoga","tohir")
fungsi_proses <- function(a){
a %>%
casefold() %>%
rm_url(pattern = pastex("@rm_youtube_url","@rm_url")) %>%
gsub('#\\S+','',.) %>%
gsub('@\\S+','',.) %>%
gsub('[^[:alpha:] ]','',.) %>%
stri_trans_general("latin-ascii") %>%
stri_replace_all_fixed(old,new,vectorize_all = FALSE) %>%
removeWords(stopwords(language = "id", source = "nltk")) %>%
removeWords(hapus)
}
data <- read.csv("Data.csv")
dtnew <- mutate(data,komentar_bersih=sapply(textDisplay,fungsi_proses))
dttoken <- dtnew %>%
select(komentar_bersih) %>%
unnest_tokens(kata,komentar_bersih) %>%
count(kata) %>%
top_n(12)
dt1 <- data %>% select(authorDisplayName,textDisplay)
dt1$authorDisplayName <- paste0('@',dt1$authorDisplayName)
dt2 <- dt1 %>%
unite(teks,authorDisplayName,textDisplay,sep=" ")
dt3 <- str_extract_all(dt2$teks,"(@[[:alnum:]_]*)")
dt3 <- sapply(dt3,paste,collapse=" ")
dt3 <- data.frame(dt3)
dt3$count <- str_count(dt3$dt3,"\\S+")
dt4 <- dt3 %>% filter(count > 1)
dt5 <- dt4 %>%
select(dt3) %>%
unnest_tokens(username,dt3,token = "ngrams",n=2)
dt6 <- dt5 %>%
separate(username,into=c("source","target"),sep=" ")
dt6$source <- paste0('@',dt6$source)
dt6$target <- paste0('@',dt6$target)
ig <- graph_from_data_frame(dt6,directed=FALSE)
plot(ig,layout=layout_with_kk,vertex.size=3,vertex.label=NA)
library(networkD3)
simpleNetwork(dt6)
write_graph(ig,"ajsk8.graphml",format="graphml")
library(dplyr)
library(stringr)
library(tm)
library(stopwords)
library(katadasaR)
library(stringi)
library(qdapRegex)
library(igraph)
library(networkD3)
library(readxl)
library(tidytext)
library(writexl)
library(tidyr)
# kata normalisasi
old <- c(" ni "," tu "," org "," yg "," ga ","ambik"," klo "," ngga ","tdk", " nggak ", " ya ")
new <- c(" ini "," itu "," orang "," yang "," tidak ","ambil"," kalau "," tidak "," tidak ", " tidak "," iya ")
#butuh dihapus
hapus <- c("aja","erick","erik", "gak","nya","iya","orang","semoga","tohir")
fungsi_proses <- function(a){
a %>%
casefold() %>%
rm_url(pattern = pastex("@rm_youtube_url","@rm_url")) %>%
gsub('#\\S+','',.) %>%
gsub('@\\S+','',.) %>%
gsub('[^[:alpha:] ]','',.) %>%
stri_trans_general("latin-ascii") %>%
stri_replace_all_fixed(old,new,vectorize_all = FALSE) %>%
removeWords(stopwords(language = "id", source = "nltk")) %>%
removeWords(hapus)
}
data <- read.csv("Data.csv")
dtnew <- mutate(data,komentar_bersih=sapply(textDisplay,fungsi_proses))
dttoken <- dtnew %>%
select(komentar_bersih) %>%
unnest_tokens(kata,komentar_bersih) %>%
count(kata) %>%
top_n(12)
dt1 <- data %>% select(authorDisplayName,textDisplay)
dt1$authorDisplayName <- paste0('@',dt1$authorDisplayName)
dt2 <- dt1 %>%
unite(teks,authorDisplayName,textDisplay,sep=" ")
dt3 <- str_extract_all(dt2$teks,"(@[[:alnum:]_]*)")
dt3 <- sapply(dt3,paste,collapse=" ")
dt3 <- data.frame(dt3)
dt3$count <- str_count(dt3$dt3,"\\S+")
dt4 <- dt3 %>% filter(count > 1)
dt5 <- dt4 %>%
select(dt3) %>%
unnest_tokens(username,dt3,token = "ngrams",n=2)
dt6 <- dt5 %>%
separate(username,into=c("source","target"),sep=" ")
dt6$source <- paste0('@',dt6$source)
dt6$target <- paste0('@',dt6$target)
ig <- graph_from_data_frame(dt6,directed=FALSE)
plot(ig,layout=layout_with_kk,vertex.size=3,vertex.label=NA)
library(networkD3)
simpleNetwork(dt6)
write_graph(ig,"ajs.graphml",format="graphml")
